{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport zipfile\nimport os\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom PIL import Image\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom skimage.metrics import structural_similarity as ssim\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2024-09-01T17:30:58.361050Z","iopub.execute_input":"2024-09-01T17:30:58.362652Z","iopub.status.idle":"2024-09-01T17:31:05.267802Z","shell.execute_reply.started":"2024-09-01T17:30:58.362564Z","shell.execute_reply":"2024-09-01T17:31:05.266181Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/Mvryo02/Plane-anomaly-detection-.git","metadata":{"execution":{"iopub.status.busy":"2024-09-01T17:31:10.695289Z","iopub.execute_input":"2024-09-01T17:31:10.695994Z","iopub.status.idle":"2024-09-01T17:31:31.910336Z","shell.execute_reply.started":"2024-09-01T17:31:10.695944Z","shell.execute_reply":"2024-09-01T17:31:31.908410Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Cloning into 'Plane-anomaly-detection-'...\nfatal: unable to access 'https://github.com/Mvryo02/Plane-anomaly-detection-.git/': Could not resolve host: github.com\n","output_type":"stream"}]},{"cell_type":"code","source":"#calcolare mean e standard deviation da sostituire in transform.Normalize di train\ntemporal_transform = transforms.Compose([\n    transforms.ToTensor()\n])\n\nos.chdir(\"/content/Plane-anomaly-detection-/\")\n\ndataset = datasets.ImageFolder(root='./train', transform=temporal_transform)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n\nmean = 0.0\nstd = 0.0\nnb_samples = 0\n\nfor data in dataloader:\n    imgs, _ = data\n    batch_samples = imgs.size(0)\n    imgs = imgs.view(batch_samples, imgs.size(1), -1)\n    mean += imgs.mean(2).sum(0)\n    std += imgs.std(2).sum(0)\n    nb_samples += batch_samples\n\nmean /= nb_samples\nstd /= nb_samples\n\nprint(f'Mean: {mean}')\nprint(f'Std: {std}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bilateral_filter(image):\n    np_image = np.array(image)  # Convert PIL image to numpy array\n    filtered = cv2.bilateralFilter(np_image, d=9, sigmaColor=75, sigmaSpace=75)\n    return Image.fromarray(filtered)  # Convert back to PIL image\n\ndef sharpen(image):\n    np_image = np.array(image)  # Convert PIL image to numpy array\n    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n    sharpened = cv2.filter2D(np_image, -1, kernel)\n    return Image.fromarray(sharpened)  # Convert back to PIL image\n\nclass CustomTransform:\n    def __init__(self, additional_transform=None):\n        self.additional_transform = additional_transform\n\n    def __call__(self, img):\n        img = bilateral_filter(img)  # Applicare il filtro bilaterale\n        img = sharpen(img)  # Applicare il filtro di sharpening\n        if self.additional_transform:\n            img = self.additional_transform(img)\n        return img\n\ntransform_train = transforms.Compose([\n    CustomTransform(),                                                              # Applicare filtri personalizzati\n    transforms.RandomHorizontalFlip(p=0.5),                                         # Flip orizzontale casuale per data augmentation\n    transforms.RandomRotation(15),                                                  # Rotazione casuale di 15 gradi per data augmentation\n    #transforms.ColorJitter(brightness=0.1, contrast=0.1),                           # Modifica casuale dei colori per data augmentation\n    transforms.Resize((800,800)),                                                    # Ridimensiona l'immagine a 800x800\n    transforms.ToTensor(),                                                              # Converte l'immagine in tensor\n    transforms.Normalize(mean=[0.4743, 0.5177, 0.5279], std=[0.1772, 0.1736, 0.1722]),  # Normalizza l'immagine con dati trovati\n])\n\ntransform_test = transforms.Compose([\n    CustomTransform(),  # Applicare filtri personalizzati\n    transforms.Resize((800,800)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvAutoencoder(nn.Module):\n    def __init__(self):\n        super(ConvAutoencoder, self).__init__()\n\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),  # 800x800 -> 400x400\n            nn.ReLU(True),        # ReLU(True) activation function: modifica l'input originale per risparmiare memoria e leggermente più veloce.\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # 400x400 -> 200x200\n            nn.ReLU(True),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), # 200x200 -> 100x100\n            nn.ReLU(True)\n        )\n\n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # 100x100 -> 200x200 #output_padding: Aggiunge pixel extra all'output della convoluzione trasposta per raggiungere una dimensione spaziale specifica. Serve per garantire che l'output abbia esattamente la dimensione desiderata, correggendo eventuali discrepanze che potrebbero derivare dall'uso di stride e padding.\n            nn.ReLU(True),\n            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # 200x200 -> 400x400\n            nn.ReLU(True),\n            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, padding=1, output_padding=1),   # 400x400 -> 800x800\n            nn.Sigmoid()   # Per mappare l'output nello stesso range [0, 1] delle immagini in input\n        )\n\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(\"/content/Plane-anomaly-detection-/\")\n\ntrain = datasets.ImageFolder(root='./train', transform=transform_train)\ntrainloader = DataLoader(train, batch_size=32, shuffle=True, pin_memory=True, num_workers=2)\nnum_elements = len(train)\nprint(f\"Numero di immagini per il train nel dataset: {num_elements}\")\n\ntest = datasets.ImageFolder(root='./test', transform=transform_test)\ntestloader = DataLoader(test, batch_size=32, shuffle=True, pin_memory=True)\nnum_elements = len(test)\nprint(f\"Numero di immagini per il test nel dataset: {num_elements}\")\n\n#test per vedere se immagini sono tensori e visualizzare immagini\ndef imshow(tensor_image, title=None):\n    # Converti il tensore in un array NumPy\n    image = tensor_image.numpy()\n\n    # Normalizza l'immagine se necessario (assumiamo che il range sia [-1, 1] o [0, 1])\n    if image.min() < 0:  # Normalizzazione [-1, 1] -> [0, 1]\n        image = (image + 1) / 2\n    image = np.clip(image, 0, 1)  # Assicurati che i valori siano tra 0 e 1\n\n    # Se l'immagine è in formato CHW (Channel, Height, Width), convertila in HWC (Height, Width, Channel)\n    if image.shape[0] == 3:\n        image = np.transpose(image, (1, 2, 0))\n\n    # Visualizza l'immagine\n    plt.imshow(image)\n    if title:\n        plt.title(title)\n    plt.axis('off')\n    plt.show()\n\ndata_iter = iter(trainloader)\nimages, _ = next(data_iter)  # Ottieni un batch di immagini e target\n\n# Seleziona la prima immagine del batch\nimage = images[0]\n\n# Visualizza l'immagine\nimshow(image, title='Example Image from Train Dataset')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_folds = 2 #numero di fold, metto 10 dato che dataset non è molto grande\nkf = KFold(n_splits=k_folds, shuffle=True)\n\n# Assicurati che il dispositivo sia impostato su GPU se disponibile\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Verifica se la GPU è disponibile\nif torch.cuda.is_available():\n    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"GPU is not available.\")\n\n# Funzione di training e valutazione\ndef train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, epochs):\n    for epoch in range(epochs):\n        model.train()       #modello impostato in modalita train, abilitato dropout e batch normalization\n\n        # Aggiungi questa stampa per ogni epoca per verificare il dispositivo in uso\n        print(f\"Device in use: {next(model.parameters()).device}\")\n\n        running_loss = 0.0\n        for images, _ in train_loader:  #per ogni batch di immagini in train_loader\n            images = images.to('cuda')  # Trasferire i dati sulla GPU\n            optimizer.zero_grad()       #Azzeramento dei gradienti per evitare l'accumulo\n            outputs = model(images)     #forward pass\n            loss = criterion(outputs, images) #Calcolo della perdita confrontando l'output del modello con l'input originale (in un autoencoder).\n            loss.backward()           #Calcolo del gradiente rispetto ai parametri.\n            optimizer.step()          #Aggiornamento dei parametri del modello basato sul gradiente calcolato\n            running_loss += loss.item()\n\n        model.eval()  #modello  impostato in modalità eval(), disabilitato dropout e batch normalization\n        val_loss = 0.0\n        with torch.no_grad(): #Non vengono calcolati i gradienti per rendere piu efficiente\n            for images, _ in val_loader:\n                images = images.to('cuda')  # Trasferire i dati sulla GPU\n                outputs = model(images)\n                loss = criterion(outputs, images)\n                val_loss += loss.item()   #Si calcola la val_loss (perdita sul set di validazione) per ogni batch di immagini nel val_loader\n\n        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss/len(val_loader)}')  #Al termine di ogni epoca, si stampa la perdita media sia per il set di addestramento (running_loss) che per il set di validazione (val_loss)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Itera su ciascun fold\nresults = []  #Questa lista verrà utilizzata per memorizzare la perdita media di validazione per ciascun fold. Alla fine del processo, conterrà la perdita per ogni fold.\n\nfor fold, (train_ids, val_ids) in enumerate(kf.split(train)):\n    print(f'Fold {fold+1}/{k_folds}') #fold è il fold i-esimo che va da 0 a k-fold-1\n\n    # Crea i sotto-dataset per il training e la validazione\n    train_subsampler = Subset(train, train_ids) #Subset: Crea un sotto-dataset usando gli indici forniti.\n    val_subsampler = Subset(train, val_ids) #train_subsampler contiene i dati per il training, mentre val_subsampler contiene i dati per la validazione\n\n    # Crea i dataloader per il training e la validazione\n    train_loader = DataLoader(train_subsampler, batch_size=32, shuffle=True)  #Questo insieme di dati è una porzione del dataset totale, ottenuta escludendo il fold corrente che viene usato per la validazione.\n    val_loader = DataLoader(val_subsampler, batch_size=32, shuffle=False)     #Questa è la porzione di dati che viene esclusa dall'addestramento per essere utilizzata come set di validazione in quel fold\n\n    # Inizializza il modello, la loss function e l'optimizer\n    model = ConvAutoencoder().cuda()  # Trasferire il modello sulla GPU\n    criterion = nn.MSELoss()  # La funzione di perdita è la Mean Squared Error (MSE)\n    optimizer = optim.Adam(model.parameters(), lr=0.001)  # Viene usato l'algoritmo Adam per ottimizzare i parametri del modello, con un learning rate di 0.001\n\n    # Allena e valuta\n    train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, epochs=7) #chiamata a funzione che valuta modello sopra definita\n\n    # Valutazione finale su questo fold\n    val_loss = 0.0\n    with torch.no_grad():\n        for images, _ in val_loader:\n            images = images.to('cuda')  # Trasferire i dati sulla GPU\n            outputs = model(images) #: Calcola l'output del modello dato un batch di immagini.\n            loss = criterion(outputs, images) #Calcola la perdita tra l'output del modello e l'input originale (essendo autoencoder)\n            val_loss += loss.item()\n\n    avg_val_loss = val_loss / len(val_loader) #Calcola la perdita media di validazione per questo fold\n    results.append(avg_val_loss)  # Aggiunge la perdita media di validazione alla lista results.\n    print(f'Fold {fold+1} Validation Loss: {avg_val_loss}')\n\n# Risultati finali\nprint(f'K-Fold Cross Validation results: {results}')  #Mostra la perdita di validazione media per ciascun fold\nprint(f'Average Validation Loss: {np.mean(results)}') #Calcola la media delle perdite di validazione su tutti i fold, fornendo una stima complessiva delle prestazioni del modello.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#deinizione funzione per vedere immagine e ricostruzione effettuata da autoencoder\n\ndef visualize_reconstruction(model, test_loader, num_images=5):\n    model.eval()\n    images, _ = next(iter(test_loader))\n    with torch.no_grad():\n        outputs = model(images)\n\n    outputs = torch.clamp(outputs, 0, 1) # Questo passaggio garantisce che tutti i valori dei pixel siano compresi tra 0 e 1, evitando così il problema (warning) con la visualizzazione.\n    images = torch.clamp(images, 0, 1)\n\n    fig, axes = plt.subplots(num_images, 2, figsize=(10, num_images * 2))\n    for i in range(num_images):\n        axes[i, 0].imshow(images[i].permute(1, 2, 0).cpu().numpy()) #Permuta le dimensioni dell'immagine da (C, H, W) a (H, W, C) per renderle compatibili con matplotlib, che si aspetta che i dati di immagine siano nel formato (altezza, larghezza, canali).\n        axes[i, 0].set_title(\"Original\")\n        axes[i, 0].axis('off')\n\n        axes[i, 1].imshow(outputs[i].permute(1, 2, 0).cpu().numpy())\n        axes[i, 1].set_title(\"Reconstructed\")\n        axes[i, 1].axis('off')\n\n    plt.show()\n\n# Visualizza le ricostruzioni\nvisualize_reconstruction(model, testloader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#definizione della funzione per valutare modello sul test set\ndef test_model(model, test_loader, criterion):\n    model.eval() #imposta il modello in modalità di valutazione. Questo è importante perché disabilita comportamenti specifici del training come il dropout o la batch normalization, che sono attivi solo durante l'addestramento.\n    test_loss = 0.0 #Questa variabile accumulerà la perdita su tutti i batch di test.\n    with torch.no_grad():\n        for images, _ in test_loader: # Il ciclo for itera su tutti i batch del test_loader, images: Contiene un batch di immagini del set di test.\n            outputs = model(images) #Il modello prende come input le immagini e restituisce l'output ovvero una ricostruzione dell'immagine.\n            loss = criterion(outputs, images) #La loss viene calcolata confrontando l'output del modello con l'immagine originale\n            test_loss += loss.item()  #Il valore della loss viene sommato a test_loss per accumulare la perdita totale su tutti i batch.\n\n    avg_test_loss = test_loss / len(test_loader)  #La perdita media viene calcolata dividendo la perdita totale accumulata (test_loss) per il numero di batch (len(test_loader)\n    print(f'Average Test Loss: {avg_test_loss:.4f}')\n    return avg_test_loss  #Questa metrica rappresenta la media delle perdite calcolate per ciascun esempio nel test set\n\n\n#chiamata a funzione di valutazione modello su test\nfinal_test_loss = test_model(model, testloader, criterion)  #Questo esegue la funzione test_model e assegna la avg_test_loss calcolata alla variabile final_test_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Funzione per calcolare l'errore di ricostruzione (MSE) e SSIM tra immagine originale e ricostruita\n#MSE (Mean Squared Error): Misura la differenza quadratica media tra i valori dei pixel delle immagini originale e ricostruita.\n#SSIM (Structural Similarity Index): È una metrica che misura la similarità tra due immagini, tenendo conto della luminanza, contrasto e struttura.\n\ndef calculate_errors(original, reconstructed):\n    mse = F.mse_loss(reconstructed, original).item()  #calcolo del mse\n    print(\"Original size:\", original.shape)\n    print(\"Reconstructed size:\", reconstructed.shape)\n    ssim_index = ssim(original.cpu().numpy().transpose(1, 2, 0), reconstructed.cpu().numpy().transpose(1, 2, 0), multichannel=True, win_size=3, data_range=1.0) #calcolo del ssim\n    return mse, ssim_index\n\n# Funzione per visualizzare e identificare le immagini ben riconosciute e quelle meno\ndef evaluate_model_performance(model, test_loader, threshold_mse=0.01, threshold_ssim=0.7, num_images=10):\n    model.eval()\n    detected_anomalies = 0\n    for i, (images, _) in enumerate(test_loader):\n        if i >= num_images:\n            break\n\n        with torch.no_grad():\n            outputs = model(images)\n\n        for j in range(images.size(0)):\n            original = images[j]\n            reconstructed = outputs[j]\n\n            # Calcola l'errore di ricostruzione e SSIM\n            mse, ssim_index = calculate_errors(original, reconstructed)\n\n            # Stampa i valori di MSE e SSIM\n            print(f\"Image {j+1} in batch {i+1}: MSE = {mse:.4f}, SSIM = {ssim_index:.4f}\")\n\n            # Visualizza le immagini originali e ricostruite\n            fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n            ax[0].imshow(original.cpu().numpy().transpose(1, 2, 0))\n            ax[0].set_title('Original')\n            ax[0].axis('off')\n\n            ax[1].imshow(reconstructed.cpu().numpy().transpose(1, 2, 0))\n            ax[1].set_title(f'Reconstructed\\nMSE: {mse:.4f}, SSIM: {ssim_index:.4f}')\n            ax[1].axis('off')\n\n            plt.show()\n\n            # Verifica se l'errore supera la soglia e stampa un avviso\n            if mse > threshold_mse or ssim_index < threshold_ssim:\n                print(f'Anomaly Detected: Image {j+1} in batch {i+1}')\n                detected_anomalies += 1\n\n    print(f\"Total anomalies detected: {detected_anomalies}\")\n\n# Esegui la valutazione\nevaluate_model_performance(model, testloader, threshold_mse=0.01, threshold_ssim=0.7, num_images=10)","metadata":{},"execution_count":null,"outputs":[]}]}